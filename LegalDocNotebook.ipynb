{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d08331",
   "metadata": {},
   "source": [
    "Job 1: Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab779cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import nltk\n",
    "# Load your JSON data\n",
    "df = pd.read_json('legal_passages.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cc5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = open(\"stopwords.txt\", \"r\", encoding=\"utf-8\")\n",
    "stopwords_list = []\n",
    "\n",
    "for line in stopwords_file:\n",
    "    stopwords_list.append(line.strip())\n",
    "\n",
    "stopwords_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f768b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Luật Viên chức 2010', 1: 'Bộ Luật Dân sự 2015', 2: 'Luật Giáo dục 2019', 3: 'Luật Tổ chức viện kiểm sát nhân dân 2014', 4: 'Luật Dầu khí 2022', 5: 'Luật Thanh tra 2022', 6: 'Luật Phòng, chống bạo lực gia đình 2022', 7: 'Luật Phòng, chống ma túy 2021', 8: 'Luật Cư trú 2020', 9: 'Luật Tiếp cận thông tin 2016', 10: 'Luật An ninh mạng 2018', 11: 'Hiến pháp 2013', 12: 'Luật Du lịch 2017', 13: 'Luật Tố tụng hành chính 2015', 14: 'Luật Hôn nhân và gia đình 2014', 15: 'Luật Điện ảnh 2022', 16: 'Luật Trọng tài thương mại 2010', 17: 'Luật Thanh niên 2020'}\n"
     ]
    }
   ],
   "source": [
    "doc_dict = {}  # Initialize an empty dictionary\n",
    "\n",
    "for i, row in enumerate(df.id):\n",
    "    doc_dict[i] = row\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12e1dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'underthesea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m job_1 \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'underthesea'"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import re\n",
    "job_1 = {}\n",
    "for i, row in enumerate(df.articles):\n",
    "    for text_data in row:\n",
    "        text = text_data[\"text\"]\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            token = re.sub(r'[_!@#]', '', token).lower().strip()\n",
    "#            Check if the token is a number (digits only) or a word\n",
    "            \n",
    "            if not re.search(r'[^\\w\\s\\d]+', token):\n",
    "                if not token in stopwords_list:\n",
    "                    key = (token, doc_dict[i])\n",
    "                    \n",
    "                    if job_1.get(key):\n",
    "                        job_1[key] += 1\n",
    "                    else:\n",
    "                        job_1[key] = 1  \n",
    "print(job_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install underthesea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea3ce7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_2_mapper = {}\n",
    "for term, doc_id in job_1:\n",
    "    input_key = (term, doc_id)\n",
    "    output_key = (doc_id, job_1[input_key], 1)\n",
    "    if job_2_mapper.get(term):\n",
    "        job_2_mapper[term].append(output_key)\n",
    "    else:\n",
    "        job_2_mapper[term] = [output_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e6a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_2_reducer = {}\n",
    "for term in job_2_mapper:\n",
    "    count = 0\n",
    "    for article in job_2_mapper[term]:\n",
    "        count += article[2]\n",
    "    if count <= 16 and count >= 2:\n",
    "        job_2_reducer[term] = len(doc_dict) / count \n",
    "print(job_2_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_3 = {}\n",
    "for term, doc_id in job_1:\n",
    "    input_key = (term, doc_id) \n",
    "    job_3[input_key] = job_2_reducer[term] * job_1[input_key]\n",
    "print(job_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
